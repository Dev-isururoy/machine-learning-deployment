{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "import joblib\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('models/trained_model.pkl')  # Update with the actual path to trained model file\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv('data/processed/test.csv')   # Update with the actual path to test dataset file\n",
    "\n",
    "# Separate features and target variable\n",
    "X_test = test_df.drop(columns=['loan_approval'])\n",
    "y_test = test_df['loan_approval']\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve and AUC\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel ('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show ()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "average_precision = average_precision_score(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.step(recall, precision, color='blue', where='post', label=f'Precision-Recall Curve (AP = {average_precision:.2f})')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
